Intro: n3 and doc are intimately related, in that major langDoc sections are largely generated by SPARQL query on datastore contents. These notes are limited to characterizing and giving procedures for the generation of n3 triples and docBook <LANG>Documentation.xml files for the initial core xml database.

1. Note on n3 triples:



2. General Characterizatio of AAMA Documentation:

	0) Documentation is being assembled modularly in DocBook format using the oXygen application to apply the DocBook XSL transformations (as opposed to command line or XMLMind). We are very much interested in the recent  docBook 5.1 <assembly> - <resources> - <resource> - <topic> proposal for modular documentation, but this has as yet to be included in either the oXygen or the XMLMind framework [TO BE FOLLOWED].

	1) The modular documentation is built from a core "AAMADocumentation.xml" which XInclude-s a gradually increasing number of  "<LANG>Documentation.xml" files (along with a "dataDocumentationGuide.xml"), and a, for the moment empty, "applicationDocumentation.xml" and "datasubDocumentation.xml" (the latter for users who will submit data and/or corrections of their own).

	2) External hyperlinks are expressed on the model of :
	
			<link xlink:href="http://www.ethnologue.org">Ethnologue</link>
			
	3) Internal cross references are done with <olink>s. There has been considerable difficulty in making these work in either chunked or single_file HTML output. But at the moment (1/13/12) the following seems to work in oXygen:
	
		a) olinkdb.xml lists the constituent output documents as immediate children of <targetset>, with a single target.db [note difference in target.db output from the "HTML - chunk" transformation scenario as opposed to that generated by the "HTML -single_file" scenario; cf. below d)]. (We would like to use the  <sitemap> structure option, with multiple target.db, but cannot make oXygen put the output html in the directory structure described in sitemap -- consequently all the internal xrefs are wrong.) Note that the HTML transformations seem to work only when the transformer is Saxon6.5.5, and that a restart of oXygen seems necessary between "Chunk" and "Single File" transformatios. [Is there a question of caching parameter or variable values from previous run? ]
		
		b) to generate chunked html output:
			- use the  "Docbook HTML - Chunk dup target-only" transformation scenario first, to generate the target.db, then run "Docbook HTML - Chunk" (with parameter collect.xref.targets='no')
i
		c) to generate single-file html output:
			- [involves running "Docbook HTML run version" scenario --  with parameter values collect.xref.targets='yes' and current.docid='' -- twice
			
		d) The sample modular documentation, with language data limited to Oromo, on which the olink protocals were worked out is contained in the documentation/docBook/docBook-trial directory. This directory also contains a olinkdb.xml with a sitemap (structure given in docBook-trial/documentation), and a sample of the target.db generated by the "HTML - chunk" transformation scenario as well as the target.db generated by the "HTML -single_file" scenario.


	4)  I have organized the feature-value section with the <itemizedlist> and embedded <orderedlist) elements (<glossary> proved too complicated).  On the other hand I have used <segmentedlist> for the Paradigm Lexeme section for formatting reasons, but perhaps for uniformity I should use the same structure as in the feature-value section.

	5) The "Attested Archive Forms" table  should be realized in such a way that clicking on the paradigmLabel will yield a display of the paradigm in question; we will have to decide whether this should be implemented by a link to hard-coded term cluster (XML or even HTML), or by a canned SPARQL query which will yield the paradigm in question.

	6) The last section is simply a place marker for the phonological chart which eventually will be present for each language.


3. The following are the steps to be followed to get from initial core database <LANG>-pdgms.xml to <LANG>-pdgms(-jena).n3: 

	3.1 Earlier Command-line (datastore triples are located in files <LANG>-pdgms.n3):
		Generate <lang>-pdgms.n3 (tools/README-RDF.txt):
		In LANG/ directory:
		
			make -f ../tools/Makefile n3 F=oromo-pdgms
						and
			make -f ../tools/Makefile rdfs F=oromo-pdgms
			
		where Makefile n3/rdfs is:
			
			n3:
				java -jar C:/saxon/saxonpe9-3-0-5j/saxon9pe.jar \
				-xi \
				-s:$F.xml \
				-xsl:../../tools/pdgms2n3.xsl > $F.n3;
				rapper -i guess -o rdfxml $F.n3 > $F.rdf;

			rdfs:
				java -jar C:/saxon/saxonpe9-3-0-5j/saxon9pe.jar \
				-xi \
				-s:$F.xml \
				-xsl:../tools/pdgms2rdfs.xsl > $F.rdfs.n3;
				rapper -i guess -o rdfxml $F.rdfs.n3 > $F.rdfs.rdf;

			Note that pdgms2n3.xsl already assigns a term-ID, and has been modified to assign also a aama:pid, and a aama:pdgmName.

	3.2 oXygen-based routines (datastore triples are located in files <LANG>-pdgms-jena.n3):

		3.2.1 SOP For Transformation of trial-sample datastore (afar, akkadian-OB, arbore, beja-arteiga, kemant, oromo):
			DONE:
				2/8/12:  beja-arteiga
				2/9/12: afar, akkadian-OB, arbore
				2/10/12: kemant, oromo
			1.	Preliminary text massaging of <LANG>-pdgms.xml (in oXygen):
				a.	delete initial and final '"' from all <note>
				b.	add <prop> lang and langVar to each lexeme
				c.	change pdgmLex => lexlabel
			2.	Make lexlist: run pdgmlex2n3.xsl => <LANG>-pdgms-lex-notes.n3
			3.	Make notelist:
				a.	run pdgmnote.xsl => <LANG>-pdgms-notes.tsv, 
				b.	save <LANG>-jpdgmids.tsv as <LANG>-jpdgmids-list.tsv in Notepad++ (can't make regex work properly in oXygen!)
					i.	delete ", 
					ii.	delete aama:ID.*?aama:pid
					iii.	\n\n=>\n
					iv.	delete \taama:pdgmlabel
					v.	append to <LANG>-pdgms-notes.tsv after /NOTE
				c.	run pid-pnote.pl (to transform to n3 form and to write to <LANG>-pdgms-lex-notes.n3)
				d.	(add new rdfs:label by hand, once)
			4.	Upload <LANG>-pdgms-lex-notes.n3 to datastore
			5.	Revise <LANG>Documentation.xml
				a.	Add pdgm list
					i.	SPARQL output-pdgmlabel-prop-val.rq => <LANG>-pdgm-prop-val.tsv
					ii.	tsv => <LANG>-pdgm-prop-val.xml via pdgmlisttsv2xml.pl (add extra ../ in perl and commandline if nec)
					iii.	insert in <LANG>Documentation.xml

		3.2.2. Transformation of xml data:
			1.	run pdgms2n3.xsl, param(s): lang=<LANG>, (langVar=<LANGVAR>) => <LANG>-pdgms-jena.n3
			2.	Upload <LANG>-pdgms-jena.n3 to datastore
			3.	Make <LANG>Documentation.xml
				a.	Location: add from template
				b.	Bibliography: now, add from template; eventually, make docBook bibliography. [TODO: Generate documentation bibliography in docBook form and do bibliography by simple xref/bibref. Possible path: Zotero => BibTeX => RefDB => docBook (cf DocBookXSL, p. 255; http://refdb.sourceforge.net).]
				c.	Lexemes: 
					i.	SPARQL output-pdgm-lex.rq => <LANG>-pdgm-lex.tsv
					ii.	tsv => ./documentation/docBook/doc/languages/<LANG>/ <LANG>-pdgm-lexl.xml  via pdgmlextsv2xml.pl 
					perl ../tools/pl/pdgmlextsv2xml.pl <LANG>-pdgm-lex.tsv
					iii.	insert in <LANG>Documentation.xml
				d.	Add prop-val list:
					i.	SPARQL display-lang-props-vals.rq => <LANG>-pdgm-pvlist.tsv
					ii.	tsv =>../documentation/docBook/doc/languages/<LANG>/ <LANG>-pdgm-prop-val.xml via pdgmpvlisttsv2xml.pl 
					perl ../tools/pl/pdgmpvlisttsv2xml.pl <LANG>-pdgm-pvlist.tsv
					iii.	insert in <LANG>Documentation.xml
				e.	Add pdgm list
					i.	SPARQL output-pdgmlabel-prop-val.rq => <LANG>-pdgm-prop-val.tsv
					ii.	tsv =>../documentation/docBook/doc/languages/<LANG>/ <LANG>-pdgm-prop-val.xml 
					via pdgmlisttsv2xml.pl (add extra ../ in perl and commandline if nec)
					perl ../tools/pl/pdgmlisttsv2xml.pl <LANG>-pdgm-prop-val.tsv
					iii.	insert in <LANG>Documentation.xml
			4.	Specific corrections and updates:
				a.	02/10/12: Fill in missing Appleyard1975 page references in Kemant notes
				b.	02/10/12: Make sure kemant-pdgms-lex-notes.n3 uploaded correctly
					i.	got error message (bad triple), corrected in oXygen, then seemed to upload without new command
					ii.	had to fill in two missing pdgm labels, make sure all paradigms in xml file are in datastore (delete and redo?)
				c.			


		3.2.3. Updating documentation:
			1.	routine to replace old with new
			2.	procedure for updating when documentation has annotations
				a.	include annotation in datastore? (then update becomes simple replacement, as above)
				b.	put new list next to old, combine by hand?
				c.	separate annotations formally from generated overviews, then:
					i.	simply replace feature if not annotated
					ii.	highlight annotated feature if some change, or if no longer in generated list
				d.	make pdgmlist from unnamed paradigms (identify 

		3.2.4. Generating serialized form
			1.	periodicity
			2.	format:
				a.	n3
				b.	text? (xml from that if desired)
 

	
2. Outputting the contents of the database in serialized n3 form.
	# Closely related to the issue of generating LANG_pdgms.n3 from LANG_pdgms.xml. Need here a xref to  "AAMA SPARQL SOP.txt#1. SERIALIZE DATASTORE" -- as follows:

	2.1 SERIALIZE DATASTORE

	2.1.1. N3 FILES

	In general a datastore in a SPARQL endpoint is contained in an optimized, server-specific, non-human-readable database (in fuseki,  a "tdb" database -- http://openjena.org/wiki/TDB). In order to distribute or share all or part of the datastore, or to upload it to a different SPARQL server, it is necessary to decide on a persistent serialization format. The one chosen here, for reasons of relative inspectability and size, is n3. If rdf-xml is preferred for uploading this can be obtained by using rapper (from /tools/Makefile):

		rapper -i guess -o rdfxml $F.n3 > $F.rdf;
		rapper -i guess -o rdfxml $F.rdfs.n3 > $F.rdfs.rdf;

	2.1.1.1 The original n3 files.

	At present (Nov., 2011), each <LANG> subdirectory of comadata contains the files used by GG and GR for uploading to a SPARQL server (Mulgara; these files were also the initial files uploaded to Fuseki). The original files for each <LANG> were transformed from a <LANG>/<LANG>-pdgms.xml file to two files <LANG>/<LANG>-pdgms.n3, <LANG>/<LANG>-pdgms.rdfs.n3 and thence respectively to <LANG>/<LANG>-pdgms.rdf, <LANG>/<LANG>-pdgms.rdfs.rdf by tools/pdgms2n3.xsl and tools/pdgms2rdfs.xsl (plus rapper). 

	In this present transitional stage a potential problem arises from the fact that GG is currently using Jena Fuseki (which implements SPARQL 1.1 Query and Update) as development environment with a test database consisting of Afar, Akkadian-OB, Arbore, Beja-Arteiga, Kemant, and Oromo; more languages will be added as the test expands.  As the test goes on, he has been correcting data errors and making other editorial changes using .ru update files. Thus, from the point of view of data, the Fuseki datastore is the most correct and up-to-date, and it is essential that GR have access to it (and that it be periodically uploaded to the Mulgara server on localhost:8080 and the [Sibawayhi] site]. However, in order to avoid conflict with the continuing use of the existing <LANG>/<LANG>-pdgms files, the newly generated n3 output files are all labeled <LANG>/<LANG>-pdgms-jena.n3 and <LANG>/<LANG>-pdgms.rdfs-jena.n3. 

	2.1.1.2 A new n3 file: <LANG>-jpdgmids.n3.  [01/02/12: the aama:pid, aama:pdgmlabel, and aama:pdgmnote will all be included in the <LANG>-pdgms-jena.n3 file]

	In addition -- for practical and theoretical reasons I need to enumerate elsewhere -- in the Fuseki datastore each paradigm term has been associated with its "base (= original input)" paradigm  via a new predicate aama:pid, and each pdgmID  is associated with a paradigm label. The latter two triples are serialized in the file <LANG>/<LANG>-jpdgmids.n3; the "termID aama:pid pdgmID"  triples however are contained in the <LANG>/<LANG>-jpdgms.n3 files. 

	The jpdgmids.n3 file is thus of the format:

	aama:<pdgmID> a aama:pid;
			aama:pdgmlabel "<PDGMLABEL>"
			.

	(e.g.:
		aama:ID0369ee0a-7c28-4b17-b84b-27d30e80a9c0 a aama:pid ;
			aama:pdgmlabel "oromo-VBaseNegImprtvSmpl-beek"
			.
	)

	while in the jpdgms.n3 files, each set of triples whose subject is the termID contains one triple associating the term with its pdgmID:

		aama:<termID> aama:pid aama:<pdgmID>

	(e.g.:

		aama:IDc98a3742-338a-4357-8f8c-82caa1c4bb14  a  aama:Term ;
			aama:dervStem	aama:base ;
			aama:gender	aama:⊤ ;
			aama:lang	aama:oromo ;
			aama:number	aama:sg ;
			aama:pdgmLex	"beek" ;
			aama:person	aama:p2 ;
			aama:pid	aama:ID0369ee0a-7c28-4b17-b84b-27d30e80a9c0 ;  # <===
			aama:polarity	aama:negative ;
			aama:pos	aama:verb ;
			aama:tam	aama:imperative ;
			aama:token	"beek-ini"
			.
	)

	2.1.1.3 A possible additional file: <LANG>-jpdgms-com-props.n3.  

	A feature of the <LANG>/<LANG>-pdgms.xml is an element <common-properties> which occurs with each <pdgm> and enumerates the property-value pairs common to all the rows of the paradigm (i.e., the col-row entries which are invariable throughout the paradigm). On the one hand these are the col-row entries which, although they are true of each term in the paradigm,  one does not want cluttering up a paradigm display. On the other hand they are (usually, although perhaps not necessarily) the prop-val pairs which distinguish a given (base-)paradigm from every other (base-)paradigm in the language (or, for that matter, in the archive, since the properties lang and langVar are, along with pos, present in each <common-properties> element). 

	In fact the unwieldy paradigm labels associated with each base-paradigm arise from an attempt to give a unique name to every base-paradigm in the archive (since originally each was contained in a separate file). This name was programmatically generated for each base-paradigm as it was extracted from the original <LANG>-pdgms.txt and transformed  to the <PDGM-NAME>.xml file. The <pdgmName> element was preserved when the pdgms were gathered into a single <LANG>-pdgms.xml file. 

	The set of prop-val pairs common to a given aama:pid (or in fact any collection of terms) can be generated on an ad-hoc basis by a (time-consuming!) SPARQL query if needed -- in fact output-common-properties.rq does just that. It remains to be seen whether it will be useful to pre-generate a set of these for each base-pdgm. Such a set of properties could be helpful: a) for paradigm display, or b) for narrowing down the search for a specific default paradigm -- i.e., a display of the different subject-agreement shapes for a basic (unique) combination of theme-tense-aspect-mode-inflectionClass values; or c) perhaps even getting rid entirely of the unwieldy "<PDGMLABEL>".  If this turns out to be the case, a <LANG>-pdgm-com-props.n3 could be merged with the <LANG>-pdgmids.n3 with a resulting set of triples:
		aama:<PID> a aama:pid;
			aama:pdgmlabel "<PDGMLABEL>";
			aama:<PROP1>	aama:<VAL1>;
			aama:<PROP2>     aama:<VAL2>;
			.  .  .
			.
	or simply:
		aama:<PID> a aama:pid;
			aama:<PROP1>	aama:<VAL1>;
			aama:<PROP2>     aama:<VAL2>;
			.  .  .
			.
	where there is no PDGMLABEL, and an adhoc descriptive tag is generated as needed from the relevant  PROP-i/VAL-j combinations (e.g., within a language, probably not necessary to include aama:lang or aama:pos).

	2.1.2. Outputting the n3 files by language.

	The process of serializing the contents of the datastore as a set of n3 files follows a uniform set of  steps for each type of file:

	1.	a set of triples SELECTED by a SPARQL query are saved as a <LANG>/<LANG>-jpdgm ... .tsv (tab-separated value) file, 
	2.	which  is processed, for the moment, by a perl script, /tools/pl/pdgmstsv2n3.pl (I will get up a pdgmstsv2n3.xsl script as soon as I can get around to it!), resulting in
	3.	a <LANG>/<LANG>-jpdgm ... .n3 file.

		
	The perl script works uniformly for each type of tsv file. In the following breakdown I identify the SPARQL query file (all of which are in the /tools/aamaTest/rq-ru sub-directory), the TSV output file input to the perl script, and the final N3 file.

	2.1.2.1 The pdgm triples:
		SPARQL: 	output-pdgms.rq
		TSV file:		<LANG>/<LANG>-jpdgms.tsv
		N3 file:		<LANG>/<LANG>-jpdgms.n3

	2.1.2.2 The property and value labels:
		SPARQL:	output-pdgms.rdfs.rq
		TSV file:		<LANG>/<LANG>-jpdgms.rdfs.tsv
		N3 file:		<LANG>/<LANG>-jpdgms.rdfs.n3

	2.1.2.3 The base-paradigm information:
		SPARQL:	output-pdgmids.rq
		TSV file:		<LANG>/<LANG>-jpdgmids.tsv
		N3 file:		<LANG>/<LANG>-jpdgmids.n3

	(Note here that there is an alternative query, output-pdgmids-alt.rq, which might eventually be useful in itself, but whose output can't be handled by pdgmstsv2n3.pl.)

	2.1.2.4 The common-properties information (if decided to include):
		SPARQL:	output-common-properties.rq
		TSV file:		<LANG>/<LANG>-jpdgms-com-props.tsv
		N3 file:		<LANG>/<LANG>-jpdgms-com-props.n3

	(Note that the resulting n3 file, if desired, could be combined with the jpdgmids.n3 through a clean-up script which essentially repeats some of the stages of the pdgmstsv2n3.pl script). The essential  method of the common-properties.rq is to get all the properties of a given paradigm, and then subtract those properties that have more than one value w/i the paradigm. There is a query which will output the non-common properties of a paradigm, output-non-common-properties.rq, and which is essentially the complement of the output-common-properties.rq query. It is in fact essentially what is used in the MINUS{ } clause of that query.)
